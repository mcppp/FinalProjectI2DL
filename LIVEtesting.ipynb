{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "LIVEtesting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcppp/FinalProjectI2DL/blob/main/LIVEtesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7vIyaVOcIzp",
        "outputId": "3903dc52-03c3-4a98-a1bf-f6571bb2ffff"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image \n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "print(torch.__version__)\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwFufiAccIzs"
      },
      "source": [
        "data_path = './FER2013' #make sure path is correct "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o27nm3scIzt"
      },
      "source": [
        "class FERdataset(Dataset):\n",
        "    def __init__(self, data_path, is_training):\n",
        "        self.data_path = data_path\n",
        "        self.train_path = os.path.join(data_path, 'train') #directory path for train set\n",
        "        self.val_path = os.path.join(data_path, 'validate') #directory path for validation set\n",
        "        self.is_training = is_training\n",
        "        if self.is_training:  #based on flag is_training, the target_path will be assigned\n",
        "            self.target_path = self.train_path\n",
        "        else:\n",
        "            self.target_path = self.val_path\n",
        "\n",
        "        self.classes = sorted(os.listdir(self.target_path)) \n",
        "        self.img_path_label = list() #creating empty list \n",
        "        for c in self.classes: #for all classes found in directory\n",
        "            img_list = os.listdir(os.path.join(self.target_path, c)) #list of img names found in target_path\n",
        "            for fp in img_list: #for each img in each class folder\n",
        "                full_fp = os.path.join(self.target_path, c, fp)  #save full path of each image\n",
        "                self.img_path_label.append((full_fp, c, self.classes.index(c))) #fill/make list of full file path of img, class name, index of class required to train nn\n",
        "    \n",
        "        self.tensor_transform = torchvision.transforms.ToTensor() #converts img to tensor type\n",
        "               \n",
        "    def __len__(self):\n",
        "        return len(self.img_path_label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        (fp, class_name, class_label) = self.img_path_label[idx]\n",
        "        img = Image.open(fp)\n",
        "        original_img = self.tensor_transform(img)  #make original img into tensor\n",
        "\n",
        "        input = self.tensor_transform(img) #applying defined transformations to validation data\n",
        "            \n",
        "        sample = dict()   #creating dict\n",
        "        sample['input'] = input #our transformed image\n",
        "        sample['original_img'] = original_img  #original image\n",
        "        sample['target'] = class_label\n",
        "        sample['class_name'] = class_name\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J5WOlRHcIzu"
      },
      "source": [
        "train_dataset = FERdataset(data_path, True)\n",
        "val_dataset = FERdataset(data_path, False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIGXk7uqcIzu"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "val_set, test_set = torch.utils.data.random_split(val_dataset, [3589, 3589])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGejWM1McIzu"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True) #DataLoader is for batching, etc. (sampling)\n",
        "\n",
        "val_dataloader = DataLoader(val_set, batch_size=1, shuffle=False, pin_memory=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)\n",
        "\n",
        "num_classes = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcRE4Y1BcIzv"
      },
      "source": [
        " device = 'cpu'\n",
        "#device = 'cuda'\n",
        "#print('Current Device : {}'.format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8-27P8ZcIzv"
      },
      "source": [
        "#how to import model from torch\n",
        "class Model(nn.Module): \n",
        "    def __init__(self, feat_dim = 2048, dim_output=7):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.feat_dim = feat_dim #dim of feature after getting flattened into a vector (we check this, we do not know it in advance)\n",
        "        self.dim_output = dim_output #output dimension = number of classes\n",
        "        \n",
        "\n",
        "        ###################################### PRE-TRAINED MODEL ###########################################\n",
        "        self.backbone = torchvision.models.resnet152(pretrained=True) #download model that's already been trained based on image classification problem\n",
        "        \n",
        "        for p in list(self.backbone.children())[:-1]:\n",
        "            p.requires_grad = False  #parameters of layers inside list in line above become fixed\n",
        "                                     #True: parameter will be trained, False: parameter won't be trained, frozen\n",
        "          \n",
        "        # # # get the structure until the Fully Connected Layer\n",
        "        modules = list(self.backbone.children())[:-1]  #list of all layers except last one (the fc one)\n",
        "        self.backbone = nn.Sequential(*modules) #put list into nn mode, not just a list -> this is our new model without last layer\n",
        "\n",
        "        \n",
        "        ######################################## DEFINING MY NEW LAYERS ####################################################\n",
        "        self.fc1 = nn.Linear(feat_dim, feat_dim//2) # 2048 -> 1024 \n",
        "                                                    # we know input has to be 2048 because that was the input of the fc we deleted from pre-trained model\n",
        "        self.fc2 = nn.Linear(feat_dim//2, feat_dim//4) # 1024 -> 512\n",
        "        self.fc3 = nn.Linear(feat_dim//4, dim_output) # 512 -> 7 must be the output dim, it is the number of classes (pokemon types)\n",
        "       \n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.LeakyReLU(0.1, inplace=True)\n",
        "\n",
        "        \n",
        "        ######################################## STRUCTURE OF FINAL MODEL ####################################################\n",
        "\n",
        "    def forward(self, img):  \n",
        "        batch_size = img.shape[0] \n",
        "        out = self.backbone(img)       \n",
        "        out = out.view(batch_size, -1)      \n",
        "        out = self.fc1(out)\n",
        "        out = self.dropout(self.relu(out)) \n",
        "        out = self.fc2(out) \n",
        "        out = self.dropout(self.relu(out)) \n",
        "        out = self.fc3(out) \n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3109FQfGcIzw"
      },
      "source": [
        "model = Model() # creating model\n",
        "model = model.to(device) # to train on GPU\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4) #define optimizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGR9LJt8cIzw"
      },
      "source": [
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.1) #learning rate scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVhqUjIocIzx"
      },
      "source": [
        "#we only need validation function for reporting on unseen test set (we are not training in this notebook)\n",
        "\n",
        "def validate(model, sample):\n",
        "    model.eval()  #turning on evaluatino time\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() #loss function\n",
        "\n",
        "    with torch.no_grad(): # turning off gradient computation when we know we won't call tensor.backward()\n",
        "\n",
        "        inp = np.repeat(sample['input'].squeeze()[..., np.newaxis], 3, -1)\n",
        "        inp = torch.Tensor(inp)\n",
        "        inp = torch.unsqueeze(inp, 0)\n",
        "        inp = inp.permute(0,3,1,2)        \n",
        "       \n",
        "        input=inp.float().to(device) # our transformed image batch; torch.Size([64, 3, 224, 224])\n",
        "        target = sample['target'].long().to(device) # class label batch; torch.Size([64])\n",
        "\n",
        "        pred = model(input)\n",
        "        pred_loss = criterion(pred, target)\n",
        "\n",
        "        top3_val, top3_idx = torch.topk(pred, 3)\n",
        "\n",
        "        num_correct = torch.sum(top3_idx == target.view(-1, 1))\n",
        "\n",
        "    return pred_loss.item(), num_correct.item(), pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWDk5FwHcIzx",
        "outputId": "1e2bbdf4-ac8d-45c7-d0fa-3dd5f799e3cb"
      },
      "source": [
        "#loading what we already trained\n",
        "best_path=torch.load('./face_11.pth',map_location=torch.device('cpu'))\n",
        "model.load_state_dict(best_path['model_state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4d8f743610e34a84aca08e328e225ad6"
          ]
        },
        "id": "CI7IAQQWcIzy",
        "outputId": "b47a19fa-ab7b-484e-ef1f-779cb3807995"
      },
      "source": [
        "# Reporting accuracy on test set\n",
        "test_loss = 0.0\n",
        "test_accu = 0.0\n",
        "\n",
        "    # Iterate over the val_dataloader\n",
        "with tqdm(total=len(test_dataloader)) as pbar: # SUB PROGRESS BAR 2, up to 586 because that's the number of images in the 1 batch of val_dataloader\n",
        "    for idx, sample in enumerate(test_dataloader): # for each of the 586 images\n",
        "            curr_loss, num_correct,_ = validate(model, sample)\n",
        "            test_loss += curr_loss / len(test_dataloader) # average validation loss\n",
        "            test_accu += num_correct / len(test_dataloader) # average accuracy\n",
        "            pbar.update(1) # update SUB PROGRESS BAR 2\n",
        "            \n",
        "print(test_loss, test_accu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d8f743610e34a84aca08e328e225ad6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=3589), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1.1537466278913924 0.8988576205070934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXk8tS-McIzy",
        "outputId": "86a76291-7111-4351-c559-a002666f8ae3"
      },
      "source": [
        "print('Reported test accuracy: ', test_accu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reported test accuracy:  0.8988576205070934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi_uKDhScIzy",
        "outputId": "cd1099bf-ff64-47b7-c297-376af102be86"
      },
      "source": [
        "##Live testing\n",
        "\n",
        "results={0:'angry', 1:'disgust', 2:'fear', 3:'happy', 4:'neutral', 5:'sad', 6:'surprise'}\n",
        "\n",
        "rect_size = 4\n",
        "cap = cv2.VideoCapture(0) \n",
        "\n",
        "cascPath = os.path.dirname(cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
        "haarcascade = cv2.CascadeClassifier(cascPath)\n",
        "\n",
        "while True:\n",
        "    (rval, im) = cap.read()\n",
        "    im=cv2.flip(im,1,1) \n",
        "\n",
        "    \n",
        "    rerect_size = cv2.resize(im, (im.shape[1] // rect_size, im.shape[0] // rect_size))\n",
        "    faces = haarcascade.detectMultiScale(rerect_size)\n",
        "    for f in faces:\n",
        "        (x, y, w, h) = [v * rect_size for v in f] \n",
        "        \n",
        "        face_img = im[y:y+h, x:x+w]\n",
        "     \n",
        "        rerect_sized=cv2.resize(face_img,(48,48))\n",
        "  \n",
        "        normalized=rerect_sized/255.0 \n",
        "        \n",
        "        rgb_weights = [0.2989, 0.5870, 0.1140]\n",
        "        gray = np.dot(normalized[...,:3], rgb_weights)\n",
        "        gray_tripled = np.repeat(gray[..., np.newaxis], 3, -1)\n",
        "        \n",
        "        gray_tripled = torch.Tensor(gray_tripled)\n",
        "        unorg = torch.unsqueeze(gray_tripled, 0)\n",
        "        org = unorg.permute(0,3,1,2)  \n",
        "              \n",
        "        result=model(org.float())\n",
        "\n",
        "        label1=torch.argmax(result, dim=1)[0]\n",
        "        label=label1.detach().item()\n",
        "      \n",
        "        cv2.rectangle(im,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "        cv2.putText(im, results[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,0,0),2)\n",
        "\n",
        "    cv2.imshow('LIVE',   im)\n",
        "    key = cv2.waitKey(10)\n",
        "    \n",
        "    if key == 27: \n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-22-71ce019cb2e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mrerect_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mrect_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mrect_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhaarcascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrerect_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9udi-F-ycIzz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXFvdfjNcIz0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}